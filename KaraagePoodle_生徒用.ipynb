{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgS6Ia4MaUMI"
      },
      "source": [
        "# 人工知能・深層学習実験 第６・７回\n",
        "\n",
        "コンピュータビジョンでは，人間が視覚を通して周囲の世界を認識するように，コンピュータによって自動的に画像や動画の持つ意味を認識することを目指します．  \n",
        "\n",
        "今回の実験では，コンピュータビジョン分野の中で「人間には簡単だが機械には難しい」問題の例として半ばジョーク的に挙げられることの多い唐揚げ・プードル分類に挑戦しましょう．\n",
        "\n",
        "参考：唐揚げとプードル以外にも，マフィンとチワワなどが有名です．  \n",
        "https://front-row.jp/_ct/17135189\n",
        "\n",
        "参考：実装にあたり，以下の記事を参考にしました．\n",
        "https://qiita.com/katsujitakeda/items/f1842f5e831bb6475ba8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r39D9jHQf4m2"
      },
      "source": [
        "# 実験概要\n",
        "実験は以下の流れで進めます．\n",
        "\n",
        "1.   スクレイピングによる画像データの収集\n",
        "2.   データセットの構築とデータの前処理\n",
        "3.   畳み込みニューラルネットワークの構築\n",
        "4.   画像分類の学習と評価\n",
        "5.   1～3の改善による分類性能の向上\n",
        "6.   分類性能のテスト\n",
        "\n",
        "6のテストはコンペ形式で，2回目の最後に各自のモデルの分類性能を競ってもらいます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URPy8nt7Gocf"
      },
      "source": [
        "# 0. 下準備\n",
        "google colabで動作確認しています．\n",
        "ローカル環境でうまく動かない場合は，colabに切り替えてみてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmlgUBJA51g7"
      },
      "source": [
        "## 必要なライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJeRSf3m5udA",
        "outputId": "14106ac4-c2de-4beb-d3b3-eae138bf982f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.9-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from icrawler) (4.12.3)\n",
            "Collecting bs4 (from icrawler)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from icrawler) (4.9.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from icrawler) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from icrawler) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from icrawler) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from icrawler) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->icrawler) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->icrawler) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->icrawler) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->icrawler) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->icrawler) (2024.6.2)\n",
            "Installing collected packages: bs4, icrawler\n",
            "Successfully installed bs4-0.0.2 icrawler-0.6.9\n",
            "Collecting japanize_matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from japanize_matplotlib) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->japanize_matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->japanize_matplotlib) (1.16.0)\n",
            "Building wheels for collected packages: japanize_matplotlib\n",
            "  Building wheel for japanize_matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize_matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120257 sha256=6c9aedecac70655e69b2f5c6e5962f1a997c8d0ea88d4ce9c0402614c6fc60d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/7a/6b/df1f79be9c59862525070e157e62b08eab8ece27c1b68fbb94\n",
            "Successfully built japanize_matplotlib\n",
            "Installing collected packages: japanize_matplotlib\n",
            "Successfully installed japanize_matplotlib-1.1.3\n"
          ]
        }
      ],
      "source": [
        "# シェルコマンドを使用\n",
        "!pip install icrawler\n",
        "!pip install japanize_matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AonEZb3t5y38"
      },
      "source": [
        "## 必要なライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe5w8C4Qd9IK"
      },
      "outputs": [],
      "source": [
        "# 基本的なライブラリ\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 画像のスクレイピング\n",
        "from icrawler.builtin import GoogleImageCrawler # Google経由で画像を拾ってくるライブラリ\n",
        "\n",
        "# 画像データの処理\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# ネットワークの構築，学習\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "\n",
        "# 可視化関連\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "\n",
        "# Google driveのマウント（コンペで使用）\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrqZYXhr5_R2"
      },
      "source": [
        "## シードの固定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clwuF2tM5H4c"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WPB3fVuGkbA"
      },
      "source": [
        "# 1. スクレイピングによる画像データの収集\n",
        "\n",
        "上でインポートした*GoogleImageCrawler*を使ってスクレイピングを実行し，指定したディレクトリに画像を保存します．\n",
        "\n",
        "参考（icrawlerのドキュメント）: https://icrawler.readthedocs.io/en/latest/builtin.html\n",
        "\n",
        "デフォルトでは，`'dataset'`というディレクトリ以下に各検索キーワードのディレクトリが作成され，その中に画像が保存されます (colab環境の場合，サイドバーの「ファイル」からファイル構成を確認できます)．\n",
        "\n",
        "スクレイピング実行時のエラーは気にしなくて良いです．\n",
        "\n",
        "演習 1.1:   以下のコードを実行し，実行結果を確認してみましょう．  \n",
        "\n",
        "演習 1.2:   スクレイピングするキーワードや枚数を変更してみましょう．  \n",
        "\n",
        "演習 1.3:   スクレイピング結果から気づいたことを議論してみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOt_bRwZeG2U"
      },
      "outputs": [],
      "source": [
        "# 画像のスクレイピング\n",
        "def scrape_imgs(keywords, max_num, save_dir='dataset'):\n",
        "    print('Scraping images...')\n",
        "    for keyword in keywords.keys():\n",
        "        if not os.path.exists(os.path.join(save_dir, keywords[keyword])):\n",
        "            os.makedirs(os.path.join(save_dir, keywords[keyword]))\n",
        "        crawler = GoogleImageCrawler(\n",
        "          downloader_threads = 4,\n",
        "          storage = {'root_dir' : os.path.join('tmp/', keywords[keyword])}\n",
        "        )\n",
        "        crawler.crawl(\n",
        "            keyword = keyword,\n",
        "            max_num = max_num,\n",
        "            filters = dict(date = ((2020,1,1),(2023,12,31))) # !!変更不可!!\n",
        "        )\n",
        "        # 拡張子をpngに統一\n",
        "        for i, filename in enumerate(glob.glob(os.path.join('tmp', keywords[keyword], '*.jpg'))):\n",
        "            img = Image.open(filename)\n",
        "            img.save(filename.replace('.jpg', '.png').replace('tmp', save_dir))\n",
        "    print(f'Done! Saved to {save_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWTu6z44zi44",
        "outputId": "5f2a53d1-91df-4b0a-8e45-882276944ce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:downloader:Exception caught when downloading file https://www.nichireifoods.co.jp/media/wp-content/uploads/2023/02/2302_03_karaage_01.jpg, error: HTTPSConnectionPool(host='www.nichireifoods.co.jp', port=443): Max retries exceeded with url: /media/wp-content/uploads/2023/02/2302_03_karaage_01.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)'))), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://www.nichireifoods.co.jp/media/wp-content/uploads/2023/02/2302_03_karaage_01.jpg, error: HTTPSConnectionPool(host='www.nichireifoods.co.jp', port=443): Max retries exceeded with url: /media/wp-content/uploads/2023/02/2302_03_karaage_01.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)'))), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://www.nichireifoods.co.jp/media/wp-content/uploads/2023/02/2302_03_karaage_01.jpg, error: HTTPSConnectionPool(host='www.nichireifoods.co.jp', port=443): Max retries exceeded with url: /media/wp-content/uploads/2023/02/2302_03_karaage_01.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)'))), remaining retry times: 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Saved to dataset\n"
          ]
        }
      ],
      "source": [
        "# スクレイピングするキーワードを定義\n",
        "keywords = {'唐揚げ': 'karaage', 'トイプードル': 'poodle'} # 2クラス分類\n",
        "# keywords = {'唐揚げ': 'karaage', 'トイプードル': 'poodle', 'コロッケ': 'croquette', 'オールドファッション': 'oldfashion', 'レッサーパンダ': 'redpanda', 'タワシ': 'scrubber'} # 多クラス分類\n",
        "\n",
        "# スクレイピングを実行\n",
        "max_num = 10 # 各キーワードの画像を最大で何枚集めたいか（実際にはエラーで取得に失敗するものがある）\n",
        "!rm -rf dataset\n",
        "scrape_imgs(keywords, max_num)\n",
        "!rm -rf tmp # 一時保存先を消去"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHWw1Wvdt430"
      },
      "source": [
        "# 2. データセットの構築とデータの前処理\n",
        "\n",
        "*PyTorch*では*Dataset*クラスが定義されており，その*Datase*tクラスを継承した新たなクラスを定義することで独自のデータセットを構築することができます．\n",
        "\n",
        "ここでは，スクレイピングで収集した画像データを用いて画像分類のためのデータセットを構築してみましょう．\n",
        "\n",
        "演習 2.1:   下記のコードがエラーなく実行できることを確かめてください．  \n",
        "\n",
        "演習 2.2:  作成したDatasetのインスタンスに対して `len()`, `show_img()`などのメソッドを実行し，結果を確認してみましょう．\n",
        "\n",
        "演習 2.3:   配布資料では前処理として画像のリサイズと画素値の正規化を行っていますが，`transform_img()`に様々な前処理を追加することでデータセットを水増し（*augmentation*）することもできます．`RandomRotation()`などのメソッドを試し，前処理によって画像の可視化結果がどうなるかを確認してください．\n",
        "\n",
        "参考（PyTorchドキュメント）: https://pytorch.org/vision/main/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1Sz3LjQqm2V"
      },
      "outputs": [],
      "source": [
        "#データセットの構築\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, class_names, img_size, img_dir='dataset/'):\n",
        "        self.classes = class_names\n",
        "        self.class_name_list = list(self.classes.keys())\n",
        "        self.label_map = {v: k for k, v in enumerate(self.classes)} # 各クラス名にIDを紐づける\n",
        "        self.img_dir = img_dir\n",
        "        self.img_size = img_size\n",
        "        self.resize = transforms.Resize(self.img_size)\n",
        "        self.img_paths, self.labels = self.make_dataset() # 各画像データの保存先とクラス名を取得する\n",
        "        self.img_paths, self.labels = self.filter_dataset() # 不適切なデータを除外し，データ形式を統一\n",
        "\n",
        "    def make_dataset(self):\n",
        "        \"\"\"\n",
        "        classes: dict\n",
        "        img_dir: path to dataset\n",
        "        \"\"\"\n",
        "        print('Making dataset...', end='')\n",
        "        img_paths = []\n",
        "        labels = []\n",
        "        for class_name, class_dir in self.classes.items():\n",
        "            class_imgs = glob.glob(os.path.join(self.img_dir, class_dir, '*.png')) # 画像を読み出す\n",
        "            img_paths.extend(class_imgs)\n",
        "            labels.extend([self.label_map[class_name]] * len(class_imgs)) # クラス名に対応するIDを付与\n",
        "        print('Done!')\n",
        "        return img_paths, labels\n",
        "\n",
        "    def filter_dataset(self):\n",
        "        \"\"\"\n",
        "        if any image is not RGB, remove it from dataset\n",
        "        img_paths: list\n",
        "        labels: list\n",
        "        \"\"\"\n",
        "        print('Filtering dataset...', end='')\n",
        "        # RGB画像でないものが混じることがあるため，除去する\n",
        "        for i, img_path in enumerate(self.img_paths):\n",
        "            img = Image.open(img_path, mode='r')\n",
        "            if img.mode != 'RGB':\n",
        "                self.img_paths.pop(i)\n",
        "                self.labels.pop(i)\n",
        "        print('Done!')\n",
        "        return self.img_paths, self.labels\n",
        "\n",
        "    def __len__(self): # データセットのサイズを定義するためのメソッド．必須．\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def read_img(self, idx):\n",
        "        # 画像の読み込み\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path, mode='r') # mode: RGB\n",
        "        img = torch.Tensor(np.array(img))\n",
        "\n",
        "        # 画像のサイズを統一\n",
        "        if img.shape[0] != 3: # チャネル数が最後に来ている場合に対処\n",
        "            img = img.permute(2, 0, 1)\n",
        "        if img.shape[0] != 3: # チャネル数が3でない場合に対処\n",
        "            img = img[:3]\n",
        "        return img, img_path\n",
        "\n",
        "    def __getitem__(self, idx): # データセットの要素を取得する際の処理を示すメソッド．必須．\n",
        "        img, img_path = self.read_img(idx) # 画像の読み込み\n",
        "        img = self.resize(img)\n",
        "\n",
        "        # 各データのラベルをone-hotベクトルとして表現\n",
        "        label = F.one_hot(torch.tensor(self.labels[idx]), num_classes=len(self.classes))\n",
        "\n",
        "        return img, label, img_path\n",
        "\n",
        "    # データセット内の画像を可視化\n",
        "    def show_img(self, idx, resize, transform, pred=''):\n",
        "        \"\"\"\n",
        "        idx: int\n",
        "        pred: str\n",
        "        \"\"\"\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path, mode='r') # mode: RGB\n",
        "\n",
        "        # resizeを可視化に反映する\n",
        "        if resize:\n",
        "            img = img.resize(self.img_size)\n",
        "\n",
        "        # transformを可視化に反映する\n",
        "        if transform:\n",
        "            img = torch.Tensor(np.array(img)).permute(2,0,1)\n",
        "            img = CollateFn(transform=True).transform_img(img)\n",
        "            img = img.permute(1,2,0).to(torch.int)\n",
        "\n",
        "        label = self.class_name_list[self.labels[idx]]\n",
        "        if pred:\n",
        "            plt.title(f'Label: {label} Pred: {pred}')\n",
        "        else:\n",
        "            plt.title(f'Label: {label}')\n",
        "        plt.imshow(img)\n",
        "\n",
        "class CollateFn:\n",
        "    def __init__(self, transform=False):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"\n",
        "        batch: list\n",
        "        \"\"\"\n",
        "        if self.transform:\n",
        "            return self.collate_fn_train(batch)\n",
        "        else:\n",
        "            return self.collate_fn_val(batch)\n",
        "\n",
        "    def collate_fn_train(self, batch):\n",
        "        \"\"\"\n",
        "        batch: list\n",
        "        \"\"\"\n",
        "        images, labels, paths = zip(*batch)\n",
        "        images = torch.stack(images)\n",
        "        images = self.transform_img(images)\n",
        "\n",
        "        labels = torch.stack(labels)\n",
        "\n",
        "        return images, labels, paths\n",
        "\n",
        "    # 画像の前処理．Composeの中にメソッドを追加することで処理を追加可能\n",
        "    def transform_img(self, img):\n",
        "        \"\"\"\n",
        "        img: torch.Tensor\n",
        "        \"\"\"\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                # 画素値の正規化．平均と分散の値はImageNetデータセットの値に準じる．\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]),\n",
        "                ## ここに新たな前処理を追加 ##\n",
        "            ])\n",
        "        return transform(img)\n",
        "\n",
        "    def collate_fn_val(self, batch):\n",
        "        \"\"\"\n",
        "        batch: list\n",
        "        \"\"\"\n",
        "        images, labels, paths = zip(*batch)\n",
        "        images = torch.stack(images)\n",
        "        # 検証データはaugmentationする必要がない\n",
        "        images = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225])(images)\n",
        "        labels = torch.stack(labels)\n",
        "\n",
        "        return images, labels, paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IruqdL8a8f2D",
        "outputId": "a029dc65-a407-4e2a-d213-67b2ea519cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making dataset...Done!\n",
            "Filtering dataset...Done!\n",
            "Class num: 2\n"
          ]
        }
      ],
      "source": [
        "# データセットの作成\n",
        "img_size = (4, 4)\n",
        "dataset = Dataset(class_names=keywords, img_size=img_size)\n",
        "class_num = len(dataset.classes)\n",
        "print(f'Class num: {class_num}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk7nzTd5WR8w"
      },
      "source": [
        "# 3. 畳み込みニューラルネットワークの構築\n",
        "コンピュータビジョン分野では，ニューラルネットワークの中でも**畳み込みニューラルネットワーク**（*CNN; Convolutional Neural Network*）と呼ばれるものが多く使われています． これは人間の視覚機能をヒントに発明されたもので，畳み込み（下記のコードでは`Conv2d()`）とプーリング（下記のコードでは`max_pool2d()`）と呼ばれる処理を繰り返すことで画像中の様々なパターンを捉えます．\n",
        "\n",
        "まずは，下記のシンプルなCNNを使って画像からクラスを分類してみましょう．\n",
        "\n",
        "演習 3.1: 下記のコードがエラーなく実行できることを確かめてください．  \n",
        "\n",
        "演習 3.2: 実行すると，データセット内の一つ目のデータに対するCNNの出力結果が表示されます．この出力は何を意味していますか？  \n",
        "\n",
        "演習 3.3: コードの中では，`Conv2d()`や`max_pool2d()`に`kernel_size`や`padding`といったオプションが設定されています．これらの意味や，他にオプションがどんなオプションがあるかを調べてみましょう．また，画像サイズやこれらのオプションを変更しても問題なく動作するようにインスタンス変数を定義してみましょう．\n",
        "\n",
        "**ヒント：畳み込み層の前後で，入出力の次元の関係は以下の式を満たします（参考: https://qiita.com/DeepTama/items/379cac9a73c2aed7a082 ）．**\n",
        "\n",
        "**出力サイズ =｛(入力サイズ + 2 × パディングサイズ - フィルタサイズ) / ストライドサイズ｝+ 1**\n",
        "\n",
        "演習 3.4: 下記のコードのCNNは，畳み込みとプーリングを2回ずつ行います．ここにコードを書き加え，畳み込みとプーリングをもう1回ずつ実行するように変更してみてください．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKORmOP99Xsu"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, class_num, img_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.conv1_outc = 4\n",
        "        self.conv1 = nn.Conv2d(3, self.conv1_outc, kernel_size=3, padding=1, stride=1) # 3 チャネルの画像 → 4 チャネルへ\n",
        "\n",
        "        self.conv1_outh = 4\n",
        "        self.conv1_outw = 4\n",
        "\n",
        "        self.pool1_size = 2\n",
        "\n",
        "        self.conv2_outc = 3\n",
        "        self.conv2 = nn.Conv2d(self.conv1_outc, self.conv2_outc, kernel_size=3, padding=1, stride=1) # 4 チャネルの画像 → 3 チャネルへ\n",
        "        self.conv2_outh = 2\n",
        "        self.conv2_outw = 2\n",
        "\n",
        "        self.pool2_size = 2\n",
        "        self.fc1_in = self.conv2_outc * 1 * 1\n",
        "        self.fc1_out = 4\n",
        "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
        "        self.fc2 = nn.Linear(self.fc1_out, class_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = F.tanh(self.conv1(x)) # 4 チャネルへ\n",
        "        assert x.shape[2] == self.conv1_outh and x.shape[3] == self.conv1_outw, 'テンソルのサイズを正しく指定してください．'\n",
        "        x = F.max_pool2d(x, kernel_size=self.pool1_size) # 画像の縦横が1/2倍に\n",
        "\n",
        "        x = F.tanh(self.conv2(x)) # 2 チャネルへ\n",
        "        assert x.shape[2] == self.conv2_outh and x.shape[3] == self.conv2_outw, 'テンソルのサイズを正しく指定してください．'\n",
        "        x = F.max_pool2d(x, kernel_size=self.pool2_size) # 画像の縦横が1/2倍に\n",
        "\n",
        "        x = x.view(batch_size, -1) # 特徴量をベクトル化\n",
        "        x = F.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrZcpNgs_MPG",
        "outputId": "45eb2686-7863-443a-f51d-b9259a459fbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 4, 4])\n",
            "tensor([[0.4673, 0.5327]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = CNN(class_num, img_size) # CNNを初期化\n",
        "model.to(device) # モデルをGPUまたはCPUに載せる\n",
        "\n",
        "sample, label, path = dataset[0]\n",
        "sample = sample.unsqueeze(0)\n",
        "sample = sample.to(device)\n",
        "print(sample.shape) # (B: バッチサイズ， C: チャネル数， H: 高さ， W: 幅)\n",
        "print(model(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9rm-ZYgCTK5"
      },
      "source": [
        "# 4. 画像分類の学習と評価\n",
        "ここまででデータセットとモデルを構築できたので，いよいよモデルの学習に移ります．\n",
        "\n",
        "今回の実験では2回目の最後にテストデータに対する分類性能を比較しますので，それまでは手元のデータを学習データと検証データに分割して使用します．\n",
        "\n",
        "演習 4.1: 以前の講義では自分自身で学習データと検証データに分割しましたが，PyTorchには`random_split()`のようにデータセットの分割を自動で行うメソッドが存在します．これを使って，学習データと検証データを8:2の割合で分割してみましょう．\n",
        "\n",
        "参考（PyTorchのドキュメント）: https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split\n",
        "\n",
        "演習 4.2: 以前の講義資料を参考に，適切な損失関数と最適化手法を設定してみましょう．\n",
        "\n",
        "演習 4.3: 以前の講義資料を参考に，モデルの出力結果から特定のクラスを予測結果に定め，その結果を基に検証データにおける正解率を算出するコードを書きましょう．\n",
        "\n",
        "演習 4.4: 下記のコードがエラーなく実行できるようにしてください．\n",
        "\n",
        "演習 4.5: 検証データの中で，予測に成功した画像と失敗した画像をそれぞれ可視化してみましょう．何か傾向は見られるでしょうか．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJywTw19_WkH"
      },
      "outputs": [],
      "source": [
        "def train(bar, model, train_loader, criterion, optimizer, epoch, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    samples = 0\n",
        "    for i, (images, labels, paths) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        # print(images.shape) # (B, C, H, W)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        # print(f\"outputs: {outputs}\")\n",
        "        # print(f\"labels: {labels}\")\n",
        "        loss = criterion(outputs, labels.to(torch.float))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        samples += images.shape[0]\n",
        "        bar.set_description(f'Epoch {epoch+1} Train {samples/len(train_loader.dataset)*100:.0f}% Loss: {loss.item()/samples:.4f}')\n",
        "    return epoch_loss/samples\n",
        "\n",
        "def val(bar, model, val_loader, criterion, epoch, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    samples = 0\n",
        "    TP = 0\n",
        "    for i, (images, labels, paths) in enumerate(val_loader):\n",
        "        samples += images.shape[0]\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels.to(torch.float))\n",
        "        epoch_loss += loss.item()\n",
        "        pred = # 予測結果を一つのクラスに定める\n",
        "        accuracy = # accuracy を算出\n",
        "        bar.set_description(f'Epoch {epoch+1} Val {samples/len(val_loader.dataset)*100:.0f}% Loss: {loss.item()/samples:.4f}, Accuracy: {accuracy:.4f}')\n",
        "    return epoch_loss/samples, accuracy\n",
        "\n",
        "def report_result(result):\n",
        "    # visualize the loss curve\n",
        "    plt.figure()\n",
        "    plt.plot(result['train'], label='train')\n",
        "    plt.plot(result['val'], label='val')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # visualize the accuracy curve\n",
        "    plt.figure()\n",
        "    plt.plot(result['acc'])\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb5OhHT_C0sh"
      },
      "outputs": [],
      "source": [
        "# 学習データと開発データを分割\n",
        "train_dataset, val_dataset = torch.utils.data.random_split() # 引数を定義し，入力する\n",
        "\n",
        "# データローダーを定義\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=False, num_workers=2, collate_fn=CollateFn(transform=True))\n",
        "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False, drop_last=False, num_workers=2, collate_fn=CollateFn(transform=False))\n",
        "\n",
        "criterion =  # 損失関数の定義\n",
        "optimizer =  # 最適化手法の定義\n",
        "\n",
        "num_epoch = 10\n",
        "print('Training...', end='')\n",
        "bar = tqdm.tqdm(range(num_epoch), leave=True)\n",
        "result = {'train': [], 'val': [], 'acc': []}\n",
        "for epoch in bar:\n",
        "    train_loss = train(bar, model, train_loader, criterion, optimizer, epoch=epoch, device=device)\n",
        "    result['train'].append(train_loss)\n",
        "    val_loss, accuracy = val(bar, model, val_loader, criterion, epoch=epoch, device=device)\n",
        "    result['val'].append(val_loss)\n",
        "    result['acc'].append(accuracy)\n",
        "print(f'Done!')\n",
        "\n",
        "torch.save(model.state_dict(), 'model.pth') # モデルを保存する\n",
        "report_result(result) # 学習中の性能の推移を可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAKmje9gHFDd"
      },
      "source": [
        "# 5. 分類性能の向上\n",
        "ここまでで，データの収集，前処理，モデルの構築，モデルの学習を経て分類性能を評価することができました．最終的に得られるモデルの分類性能は，これらすべてに左右されて決まります．ここからは，これまでに実装してきた内容に独自に工夫を加え，分類性能の向上を目指してください．\n",
        "\n",
        "演習 5.1: 収集したデータの中に，学習に悪影響を与えそうなものはありませんか．そのような画像をデータセットから除いたり，収集するデータを増やしたりするとどうなりますか．\n",
        "\n",
        "演習 5.2: 前処理を変更することで性能がどのように変化するか確認しましょう．前処理にも適切なものとそうでないものがあります．\n",
        "\n",
        "演習 5.3: モデルの構成を見直し，性能を向上させてみましょう．層を増やすだけでなく，活性化関数などの種類も見直しましょう．\n",
        "\n",
        "演習 5.4: 学習条件を見直し，性能を向上させてみましょう．最適化手法や学習のエポック数など，調整できる点が多々あります．\n",
        "\n",
        "演習 5.5: 唐揚げとプードルの分類でaccuracy 1.0 を達成しましょう．達成出来たらノートブック上部の`keywords`を多クラス分類に切り替え，そちらの性能向上を目指してください．\n",
        "\n",
        "発展 5.6: モデルの予測結果に何か傾向がないか調べてみましょう．予測に成功/失敗した画像を可視化したり，クラスごとの正解率などの統計量を算出したりといった手があります．\n",
        "\n",
        "発展 5.7: 深層学習では，過学習と呼ばれる問題がよく発生します．過学習されたモデルはトレーニングデータに対して過剰にフィットし（答えの丸暗記のようなもの），トレーニングデータに対する性能は向上しますが検証データやテストデータに対する性能が悪化します．過学習の対策方法について調べてみましょう．学習データのバリエーションを増やすことはその一つです．\n",
        "\n",
        "発展 5.8: CNNに限らずよく用いられる強力なテクニックに残差接続（*residual connection*）というものがあるので，興味がある人は調べて実装してみると良いでしょう．\n",
        "\n",
        "参考（残差接続の解説と実装）: https://euske.github.io/introdl/lec8/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foKc0YiocysU"
      },
      "source": [
        "# 6. 分類性能のテスト\n",
        "２回目の最後には，コンペ形式で各自のモデルの分類性能を評価します．\n",
        "テスト時には「唐揚げ，プードル，コロッケ，オールドファッション，レッサーパンダ，タワシ」を含んだテストデータに対して予測するので，下記の`keywords`を使ってスクレイピングと学習を実行してください．\n",
        "\n",
        "下記のコードでは，model.pathに保存したモデルを用いて，指定したディレクトリ内の画像クラスを予測します．ここでは，例として`dataset/poodle`内の画像\n",
        "を予測対象にしています．予測結果は予測対象の画像のpathとともに`pred.json`に保存されるようになっています．\n",
        "\n",
        "コンペの注意点：\n",
        "- 学習データ，検証データともに自由に増やして構いません（ただしスクレイピングの日付は変更せず，配布資料のままに統一してください）．\n",
        "- キーワードには下記の多クラス版の`keywords`を使用してください．\n",
        "- 学習後のモデルは保存しておき，下記のコードを実行するだけで`pred.json`が得られるようにしてください．\n",
        "- torchvisionなどが提供するpretrained modelの使用は認めます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sge72SbQbs2"
      },
      "outputs": [],
      "source": [
        "# keywords = {'唐揚げ': 'karaage', 'トイプードル': 'poodle', 'コロッケ': 'croquette', 'オールドファッション': 'oldfashion', 'レッサーパンダ': 'redpanda', 'タワシ': 'scrubber'}\n",
        "\n",
        "def inference(model, device, keywords, img_dir):\n",
        "    class_names = list(keywords.keys()) # 予測結果からクラス名を逆引きするためのリスト\n",
        "    pred_json = {}\n",
        "    model.eval()\n",
        "    print('Processing...')\n",
        "    bar = tqdm.tqdm(glob.glob(os.path.join(img_dir, '*.png')), leave=True, desc='Processed 0 images')\n",
        "    if len(bar) == 0:\n",
        "        raise ValueError('No img in img_dir')\n",
        "    with torch.no_grad():\n",
        "        for i, img_path in enumerate(bar):\n",
        "            img = Image.open(img_path, mode='r') # mode: RGB\n",
        "            img = torch.Tensor(np.array(img))\n",
        "\n",
        "            # 画像のサイズを統一\n",
        "            if img.shape[0] != 3: # チャネル数が最後に来ている場合に対処\n",
        "                img = img.permute(2, 0, 1)\n",
        "            if img.shape[0] != 3: # チャネル数が3でない場合に対処\n",
        "                img = img[:3]\n",
        "\n",
        "            # 推論時の前処理\n",
        "            img = transforms.Resize(img_size)(img)\n",
        "            img = transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225])(img)\n",
        "\n",
        "            img = img.unsqueeze(0)\n",
        "            img = img.to(device)\n",
        "            outputs = model(img)\n",
        "            pred = # 予測を１クラスに定める\n",
        "            pred_json[img_path] = pred\n",
        "            bar.set_description(f'{i+1} th image: {class_names[pred]}')\n",
        "\n",
        "    return pred_json\n",
        "\n",
        "model = CNN(class_num, img_size)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.to(device)\n",
        "pred_json = inference(model, device, keywords, img_dir='dataset/poodle')\n",
        "# save json\n",
        "save_path = 'pred.json'\n",
        "with open(save_path, 'w') as f:\n",
        "    json.dump(pred_json, f)\n",
        "\n",
        "print(f'\\nDone! json file was saved to {save_path}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
